# **Behavioral Cloning** 

[//]: # (Image References)

[centre_image]: ./examples/centre_image.jpg "Sample Centre Camera Image: "
[flipped_centre_image]: ./examples/flipped_centre_image.jpg "Sample Flipped Centre Camera Image: "
[left_image]: ./examples/left_image.jpg "Sample Left Camera Image: "
[right_image]: ./examples/right_image.jpg "Sample Right Camera Image: "
[loss_data]: ./examples/loss.png "Training and Validation Loss Data: "

The intention of this project is to generate and train a model to drive a car by itself in a Udacity Simulator and eventually implementing this as a real world application. I read Jeremy Shannon's [work](https://github.com/jeremy-shannon) and Vivek Sharma's [work](https://github.com/vivekmsithttps://github.com/vivekmsit) on this to understand and implement this project.

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior, yes you get to make your own dataset (Drive carefully!!)
* Build, a convolution neural network in Keras that predicts steering angles from images that you collected from driving simulator.
* Train and validate the model.
* Test that the model successfully drives around track one without leaving the road.
* Take a look at my model's code [here](https://github.com/vyaspartm/CarND-Behavioral-Cloning-P3/blob/master/model.py)

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

The project includes the following :
* model.py python script that I developed for creating and training the model.
* drive.py for driving the car in autonomous mode (Provided by Udacity)
* model.h5 containing a trained convolution neural network (I generated by running the [script](https://github.com/vyaspartm/CarND-Behavioral-Cloning-P3/blob/master/model.py))
* report_writeup.md summarizing the results (You're reading it)
* run1.mp4 video captured while running the car in autonomous mode using model.h5 (you will need [Udacity simulator](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58ae46bb_linux-sim/linux-sim.zip) for that NOTE: The link is for Linux machines)
* Training and Validation loss data in examples/loss.png
* Sample camera images and flipped image for centre camera image

#### 2. Submission includes functional code
Using the Udacity provided simulator and drive.py file, the car can be driven autonomously around the track by executing 

python drive.py model.h5

#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline used for training and validating the model. I tried to put comments in code where I can, to make is understandable.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

My model consists of a five convolution neural network layers with 5x5 and 3x3 filter sizes and depths between 24 and 64. 

The model includes RELU layers to introduce nonlinearity, and the data is normalized in the model using a Keras lambda layer. 

#### 2. Attempts to reduce overfitting in the model

The model contains dropout layer after each convolutional layer and some of the Dense layers with dropout rate of 0.1 to reduce overfitting. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.

After 7 epochs, training loss was 0.0379 and validation loss was 0.0330.
Below figure represents mapping of training and validation loss with number of epochs:

![alt text][loss_data] 

#### 3. Model parameter tuning

The model used an adam optimizer.

#### 4. Appropriate training data

Training data consists of 3 camera images left, right, centre camera images and steering angle. 

Below are the sample camera images I used (including flipped centre camera image):

Sample centre camera image:
![alt text][centre_image]

Sample flipped centre camera image:
![alt text][flipped_centre_image]

Sample left camera image:
![alt text][left_image]

Sample right camera image:
![alt text][right_image] 

While collecting training data, I used a combination of center lane driving, dropping out of road completely and recovering , recovering from the left and right sides of the road by running through same portion of the track multiple times with different steering angles.

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

The overall strategy for deriving a model architecture was to use a combination of convolutional neural network and connected networks and provide sufficient input data to the model to get correct model data.

My first step was to use a convolution neural network model similar to the traffic sign classifier project which is used for classification of input images. I thought this model might be appropriate because in behaviour cloning project also, input data is set of images and labels are the steering angles.

In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. Then for each input image, I used flipped image data to provide more input for improving accuracy of the model.

The final step was to run the simulator to see how well the car was driving around track one. There were a few spots where the vehicle fell off the track in the beginning. For those spots, I had to collect data multiple times with different steering angles. e.g. to prevent vehicle from going outside of the road on the left side, turning the vehicle right side quickly, etc. 


#### 2. Creation of the Training Set & Training Process

To capture good driving behavior, I drove the car in center lane for the most part, took the car off-road and recovered, intentionally going to the sides of road then braking and recovering.

#### 3.Finally the video

The root directory contains the cod, model and the video that was generated using images in run1 folder.
